---
title: "Podcast Generator"
description: "Create podcasts with long-form TTS"
enableComments: true
author: "Steven Lynn"
github_username: "stvlynn"
x_username: "Stv_Lynn"
demo_url: https://podcast-adp.vercel.app/
---

⬇️ Preview

<Callout title="How to use">Enter a podcast topic—such as "Technology"—to generate a one-minute show.</Callout>

<iframe
  src="https://podcast-adp.vercel.app/" 
  style={{ width: '100%', height: '600px', border: '0' }}
  allow="clipboard-write *; microphone; camera"
></iframe>


## Project Overview

This article introduces a workflow-driven intelligent podcast generator. The system combines large language models with text-to-speech services to automate every step from topic selection to audio output. A modular design supports asynchronous processing and state tracking, giving content creators a streamlined way to produce podcasts at scale.

## Business Background

Traditional podcast production involves multiple stages—script writing, recording, and post-production—and often requires professional gear and expertise. Solo creators face a high entry barrier, and consistently publishing new episodes is time-consuming.

Our project dramatically simplifies the process. Users only need to provide a topic; the system then drafts the script and converts it into audio automatically. By automating the workflow we reduce production overhead while enabling reliable, repeatable output.

## Technical Architecture

The solution is built on a workflow-centric architecture that breaks the podcast pipeline into discrete, orchestrated nodes. Core components include the DeepSeek-v3 large language model for script generation, Tencent Cloud TTS for audio synthesis, and Python utilities for data transformation and control logic.

<Mermaid
  chart="
graph TB
                %% User layer
                User[User Interface<br/>Web App]
                
                %% Workflow engine
                WFEngine[Workflow Engine]
                
                %% Parameter management
                ParamMgr[Parameter Manager]
                SecretStore[(Secret Store)]
                
                %% Core processing modules
                LLMNode[Script Generation Node]
                TTSNode[TTS Synthesis Node]
                PollingNode[Polling Controller]
                DataProc[Result Processor]
                
                %% External services
                DeepSeek[DeepSeek-v3 LLM]
                TencentTTS[Tencent Cloud TTS]
                
                %% Storage
                Cache[(Cache)]
                ResultStore[(Result Storage)]
                
                %% Sub workflow
                SubWF[Status Query Sub-workflow]
                
                %% Data flow
                User -->|1. Provide topic| WFEngine
                WFEngine -->|2. Extract params| ParamMgr
                ParamMgr -->|3. Fetch secrets| SecretStore
                
                WFEngine -->|4. Schedule| LLMNode
                LLMNode -->|5. Request script| DeepSeek
                DeepSeek -->|6. Return script| LLMNode
                LLMNode -->|7. Cache draft| Cache
                
                WFEngine -->|8. Ask for confirmation| User
                User -->|9. Confirm audio generation| WFEngine
                
                WFEngine -->|10. Schedule| TTSNode
                TTSNode -->|11. Submit task| TencentTTS
                TencentTTS -->|12. Return task ID| TTSNode
                
                WFEngine -->|13. Start polling| PollingNode
                PollingNode -->|14. Invoke sub-workflow| SubWF
                SubWF -->|15. Query status| TencentTTS
                TencentTTS -->|16. Respond status| SubWF
                SubWF -->|17. Return result| PollingNode
                
                PollingNode -->|18. Extract URL| DataProc
                DataProc -->|19. Persist output| ResultStore
                ResultStore -->|20. Share link| User
                
                %% Styles
                classDef userStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
                classDef engineStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
                classDef nodeStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
                classDef externalStyle fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
                classDef storageStyle fill:#fce4ec,stroke:#c2185b,stroke-width:2px
                
                class User userStyle
                class WFEngine,SubWF engineStyle
                class ParamMgr,LLMNode,TTSNode,PollingNode,DataProc nodeStyle
                class DeepSeek,TencentTTS externalStyle
                class SecretStore,Cache,ResultStore storageStyle
"
/>


## Core Features

### Parameter management

The workflow first extracts three required parameters: the Tencent Cloud API Secret ID, Secret Key, and the user-provided podcast topic.

Parameters are collected through a dedicated node that validates presence and type. Environment secrets are read from a secure store, while the topic is captured through user interaction and normalized for downstream tasks.

### Model orchestration

The script-generation node calls DeepSeek-v3 with a carefully designed prompt that includes the topic, target duration, tone, and any optional hints. Responses are cached so the user can review or edit before committing to synthesis.

### Asynchronous TTS processing

The TTS node submits a long-text synthesis job to Tencent Cloud. Because audio rendering is asynchronous, the workflow spawns a polling sub-workflow that checks task status at regular intervals. Results are appended to an array until a valid audio URL is returned.

<Mermaid
  chart={`
sequenceDiagram
    participant U as User
    participant WF as Workflow Engine
    participant LLM as Script Node
    participant Cache as Cache
    participant TTS as TTS Node
    participant API as Tencent TTS API
    participant P as Polling Controller
    participant SW as Sub-workflow
    participant DP as Data Processor

    U->>WF: Confirm audio generation
    WF->>TTS: Trigger TTS task
    
    rect rgb(240, 240, 255)
        note over TTS,API: Task submission
        TTS->>API: POST /CreateTtsTask<br/>{text: "Podcast script"}
        API-->>TTS: 200 OK<br/>{taskId: "task_12345"}
        TTS-->>WF: Return task ID
    end
    
    WF->>U: Show progress message<br/>"Audio is being generated..."
    
    rect rgb(255, 240, 240)
        note over P,API: Polling stage
        WF->>P: Start polling<br/>(taskId, credentials)
        
        loop Until resultURL exists
            P->>SW: Run status query
            SW->>API: POST /DescribeTtsTaskStatus<br/>{taskId: "task_12345"}
            
            alt Task processing
                API-->>SW: {status: 1, resultUrl: null}
                SW-->>P: No URL yet
                P->>P: Wait (adaptive interval)
            else Task completed
                API-->>SW: {status: 2, resultUrl: "https://..."}
                SW-->>P: Return audio URL
                note over P: Exit loop
            else Task failed
                API-->>SW: {status: 3, errorMsg: "..."}
                SW-->>P: Propagate error
                P->>WF: Trigger exception flow
            end
        end
    end
    
    rect rgb(240, 255, 240)
        note over P,U: Result processing
        P->>DP: Parse polling results
        DP->>DP: Find first non-empty URL
        DP-->>WF: Deliver audio link
        WF->>U: Display link + transcript
    end
    
    note over U,API: Async flow prevents blocking and supports resume
    `}
    />


The polling strategy increases wait intervals over time, applies a maximum retry limit, and handles transient API failures gracefully. The final URL is extracted from the aggregated polling history.

![Podcast workflow architecture](https://s2.loli.net/2025/09/25/NfIMbCs47ijuz2Q.png)

<Mermaid
  chart={`
  flowchart TD
    Start([Start Polling]) --> Init[Initialize<br/>taskId, count = 0]
    
    Init --> CheckCount{Has max attempts
    been reached?}
    
    CheckCount -->|Yes| Timeout[Timeout]
    Timeout --> End([End])
    
    CheckCount -->|No| Query[Call sub-workflow
for status]
    
    Query --> CheckStatus{Status code}
    
    CheckStatus -->|Processing| Wait[Wait interval]
    CheckStatus -->|Completed| ExtractURL[Extract audio URL]
    CheckStatus -->|Failed| Error[Return error]
    
    Wait --> Increment[Increment count<br/>store response]
    Increment --> CheckCount
    
    ExtractURL --> Success[Return result]
    Success --> End
    
    Error --> End
    
    style Start fill:#e8f5e9,stroke:#4caf50,stroke-width:2px
    style End fill:#ffebee,stroke:#f44336,stroke-width:2px
    style Success fill:#e3f2fd,stroke:#2196f3,stroke-width:2px
    style Error fill:#fff3e0,stroke:#ff9800,stroke-width:2px
    
    classDef queryNode fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px
    class Query queryNode
  `}
/>

### Error handling

The TTS node uses a progressive retry policy—up to three attempts with a 30-second backoff—to absorb transient outages. When all retries fail, the workflow returns a default response while logging detailed diagnostics for later review.

Any polling errors bubble up through a dedicated exception branch so operations teams can be alerted without interrupting the user experience.

![Error handling timeline](https://s2.loli.net/2025/09/25/ZjqCfPGVc7lvIH8.png)

## Conclusion

By orchestrating script generation and long-form TTS inside a single workflow, the project validates how AI can accelerate content creation. Modular design, asynchronous control, and resilient error handling work together to deliver stable automation. With continuous iteration, the system will keep improving and unlock even more value for creators.
